{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leaderfollower.file_helper import loadTrialData, loadConfigData, loadBatch\n",
    "from leaderfollower.data_helpers import getEvalFitnesses\n",
    "from leaderfollower.plot_helpers import plotJointTrajectory, plotBatchPerformance, PerformanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function definitions for plotting\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "from enum import IntEnum\n",
    "\n",
    "from leaderfollower.data_helpers import getEvalStatistics, getBestStatistics\n",
    "\n",
    "class PerformanceMetric(IntEnum):\n",
    "    BestTrainingTeam = 0\n",
    "    EvaluationTeam = 1\n",
    "\n",
    "def plotStatisticsAvg(avg, color):\n",
    "    num_generations_arr = np.arange(avg.shape[0])\n",
    "    plt.plot(num_generations_arr, avg, color=color, linewidth=4)\n",
    "\n",
    "def plotStatisticsRange(upper_dev, lower_dev, upper_range, lower_range, color, plot_min_max_range=False):\n",
    "    num_generations_arr = np.arange(upper_dev.shape[0])\n",
    "    plt.fill_between(num_generations_arr, upper_dev.flatten(), lower_dev.flatten(), alpha=0.2, color=color)\n",
    "    if plot_min_max_range:\n",
    "        plt.fill_between(num_generations_arr, upper_range.flatten(), lower_range.flatten(), alpha=0.2, color=color)\n",
    "\n",
    "def plotBatchPerformance(trial_datas_G: Optional[dict], trial_datas_D: Optional[dict], trial_datas_Dfollow: Optional[dict], \\\n",
    "                        plot_min_max_range: bool, computername: str, performance_metric: PerformanceMetric):\n",
    "    plt.figure(0)\n",
    "\n",
    "    num_stat_runs = len(trial_datas_G)\n",
    "\n",
    "    if performance_metric.value == PerformanceMetric.BestTrainingTeam.value:\n",
    "        getStatistics = getBestStatistics\n",
    "        title = \"Best Training Team\"\n",
    "    elif performance_metric.value == PerformanceMetric.EvaluationTeam.value:\n",
    "        getStatistics = getEvalStatistics\n",
    "        title = \"Evaluation Team\"\n",
    "\n",
    "    # Get statistics for different reward structures\n",
    "    # We plot the baselines first so that D-Indirect is on the top layer\n",
    "    legend = []\n",
    "    if trial_datas_G is not None:\n",
    "        avg_G, std_dev_G, upper_err_G, lower_err_G, upper_G, lower_G = getStatistics(trial_datas_G)\n",
    "        plotStatisticsAvg(avg_G, color='tab:blue')\n",
    "        legend.append(\"$G$\")\n",
    "        num_generations_arr = np.arange(avg_G.shape[0])\n",
    "\n",
    "    if trial_datas_D is not None:\n",
    "        avg_D, std_dev_D, upper_err_D, lower_err_D, upper_D, lower_D = getStatistics(trial_datas_D)\n",
    "        plotStatisticsAvg(avg_D, color='tab:orange')\n",
    "        legend.append(\"$D$\")\n",
    "        num_generations_arr = np.arange(avg_D.shape[0])\n",
    "\n",
    "    if trial_datas_Dfollow is not None:\n",
    "        avg_Df, std_dev_Df, upper_err_Df, lower_err_Df, upper_Df, lower_Df = getStatistics(trial_datas_Dfollow)\n",
    "        plotStatisticsAvg(avg_Df, color='tab:green')\n",
    "        legend.append(r'$D^I$')\n",
    "        num_generations_arr = np.arange(avg_Df.shape[0])\n",
    "\n",
    "    # Automatically figure out how many generations were in here\n",
    "    plt.ylim([0,1.01])\n",
    "    plt.xlim([0,len(num_generations_arr)-1])\n",
    "\n",
    "    # Add the standard error or min max plotting\n",
    "    if trial_datas_G is not None: \n",
    "        plotStatisticsRange(upper_err_G, lower_err_G, upper_G, lower_G, 'tab:blue', plot_min_max_range)\n",
    "    if trial_datas_D is not None:\n",
    "        plotStatisticsRange(upper_err_D, lower_err_D, upper_D, lower_D, 'tab:orange', plot_min_max_range)\n",
    "    if trial_datas_Dfollow is not None:\n",
    "        plotStatisticsRange(upper_err_Df, lower_err_Df, upper_Df, lower_Df, 'tab:green', plot_min_max_range)\n",
    "\n",
    "    plt.legend(legend, fontsize=20, loc=\"lower right\")\n",
    "\n",
    "    # plt.legend([\"$G$\", \"$D$\", r'$D_{follow}$'])\n",
    "\n",
    "    plt.xlabel(\"Generations\", fontsize=20)\n",
    "    plt.ylabel(\"Performance\", fontsize=20)\n",
    "    plt.xticks(ticks=None, labels=None, fontsize=15)\n",
    "    plt.yticks(ticks=None, labels=None, fontsize=15)\n",
    "    plt.grid()\n",
    "    # plt.title(title)\n",
    "\n",
    "    # plt.xlim([0,150])\n",
    "\n",
    "    plot_save_name = computername + \" | stat_runs \"+str(num_stat_runs)+\" | \"+title+\" |\"\n",
    "    if trial_datas_G:\n",
    "        plot_save_name += \" G\"\n",
    "    if trial_datas_D:\n",
    "        plot_save_name += \" D\"\n",
    "    if trial_datas_Dfollow:\n",
    "        plot_save_name += \" Df\"\n",
    "    if plot_min_max_range:\n",
    "        plot_save_name += \" | full range\"\n",
    "    else:\n",
    "        plot_save_name += \" | std err\"\n",
    "\n",
    "    png_plot_save_name = \"../../figures/png/\" + plot_save_name + \".png\"\n",
    "    svg_plot_save_name = \"../../figures/svg/\" + plot_save_name + \".svg\"\n",
    "\n",
    "    print(\"Saving plot as \", png_plot_save_name)\n",
    "    print(\"Saving plot as \", svg_plot_save_name)\n",
    "    plt.savefig(png_plot_save_name)\n",
    "    plt.savefig(svg_plot_save_name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def scaleRGB(rgb_values: List[int]):\n",
    "    scaled_values = []\n",
    "    for value in rgb_values:\n",
    "        scaled_values.append(float(value)/255.)\n",
    "    return tuple(scaled_values)\n",
    "\n",
    "def plotJointTrajectorySubplot(trial_data: List[Dict], config: Dict, ax: Axes, generation: Optional[int], team_id: int | str, individual_plot: bool = True, sample_frequency: int = 1, show_legend:bool = True):\n",
    "    if generation is None:\n",
    "        generation = config[\"num_generations\"]\n",
    "    if team_id == \"Eval\":\n",
    "        # Get the joint trajectory of the evaluation team\n",
    "        # joint_trajectory = np.array('final_evaluation_teams'[generation].joint_trajectory).tolist()\n",
    "        joint_trajectory = np.array(trial_data[generation][\"evaluation_team\"][\"joint_trajectory\"]).tolist()\n",
    "    else:\n",
    "        # First get the joint trajectory for this particular generation\n",
    "        # Each element is a snapshot of all agent positions at a particular point in time\n",
    "        # teams_in_evaluations is a global variable thanks to how python does things\n",
    "        # joint_trajectory = np.array(teams_in_evaluations[generation][team_id].joint_trajectory).tolist()\n",
    "        joint_trajectory = np.array(trial_data[generation][\"training_teams\"][\"team_\"+str(team_id)][\"joint_trajectory\"]).tolist()\n",
    "    \n",
    "    # I need the joint trajectory as a list of trajectories where each trajectory is a list of (x,y) tuples for a particular agent\n",
    "    num_trajectories = len(joint_trajectory[0])\n",
    "    list_of_trajectories = [[] for _ in range(num_trajectories)]\n",
    "    for positions in joint_trajectory:\n",
    "        for position, trajectory in zip(positions, list_of_trajectories):\n",
    "            trajectory.append(tuple(position))\n",
    "\n",
    "    # Now I need to set up variables for the plot\n",
    "\n",
    "    # Get map dimensions for figuring the x and y limits of the graph\n",
    "    map_dimensions = config[\"CCEA\"][\"config\"][\"BoidsEnv\"][\"config\"][\"map_dimensions\"]\n",
    "    map_dim_x = map_dimensions[\"x\"]\n",
    "    map_dim_y = map_dimensions[\"y\"]\n",
    "\n",
    "    # Use map dimensions to figure out correctly proportioned graph size\n",
    "    # Keep x dimension the same and adjust the y dimension accordingly\n",
    "    # fig_x = 5.\n",
    "    # fig_y = fig_x * float(map_dim_y)/float(map_dim_x)\n",
    "\n",
    "    # Get the number of leaders and followers\n",
    "    num_leaders = config[\"CCEA\"][\"config\"][\"BoidsEnv\"][\"config\"][\"StateBounds\"][\"num_leaders\"]\n",
    "    num_followers = config[\"CCEA\"][\"config\"][\"BoidsEnv\"][\"config\"][\"StateBounds\"][\"num_followers\"]\n",
    "\n",
    "    # Set up the leader colors for coloring the trajectories\n",
    "    leader_colors_rgb_raw = config[\"CCEA\"][\"config\"][\"BoidsEnv\"][\"config\"][\"Renderer\"][\"leader_colors\"]\n",
    "    leader_colors_scaled = [scaleRGB(color) for color in leader_colors_rgb_raw]\n",
    "    leader_colors = []\n",
    "    for leader_ind in range(num_leaders):\n",
    "        leader_colors.append(leader_colors_scaled[leader_ind%len(leader_colors_scaled)])\n",
    "\n",
    "    # Set up follower color\n",
    "    follower_color = scaleRGB([73, 65, 109])\n",
    "\n",
    "    # Set up leader color\n",
    "    # leader_color = leader_colors[0]\n",
    "    leader_color = scaleRGB([255, 209, 102])\n",
    "\n",
    "    # Set up colors of agents for all trajectories\n",
    "    # agent_colors = leader_colors + [follower_color]*num_followers\n",
    "\n",
    "    # I reverse it so that the leader trajectories are plotted on top\n",
    "    # for trajectory, agent_color in reversed(list(zip(list_of_trajectories, agent_colors))):\n",
    "    #     xs, ys = zip(*trajectory)\n",
    "    #     ax.plot(xs, ys, color=agent_color, marker='P')\n",
    "\n",
    "    plot_marker_size = 10\n",
    "\n",
    "    follower_trajectories = list_of_trajectories[num_leaders:]\n",
    "    for trajectory in follower_trajectories:\n",
    "        xs, ys = zip(*trajectory)\n",
    "        ax.plot(xs[::sample_frequency], ys[::sample_frequency], color=follower_color, marker=\"s\", markersize=plot_marker_size)\n",
    "\n",
    "    leader_trajectories = list_of_trajectories[:num_leaders]\n",
    "    for trajectory in leader_trajectories:\n",
    "        xs, ys = zip(*trajectory)\n",
    "        ax.plot(xs[::sample_frequency], ys[::sample_frequency], color=leader_color, marker=\"P\", markersize=plot_marker_size)\n",
    "\n",
    "    # Set up poi colors\n",
    "    # For now just use the observed color because I don't yet save \n",
    "    # if a poi has been observed or not, so to determine that we would have to do a rollout or \n",
    "    # call some code to compute that\n",
    "    poi_observed_color = scaleRGB([170, 80, 66]) #redwood color\n",
    "    # poi_observed_color = scaleRGB([229, 79, 109]) # magenta color\n",
    "    # poi_unobserved_color = scaleRGB([255, 0, 0])\n",
    "\n",
    "    # Get the POI positions for the configuration\n",
    "    # Later on this should plot the poi observation radius also\n",
    "    poi_positions = config[\"CCEA\"][\"config\"][\"BoidsEnv\"][\"config\"][\"POISpawner\"][\"positions\"]\n",
    "    num_pois = len(poi_positions)\n",
    "    poi_colors = [poi_observed_color]*num_pois\n",
    "    for poi_position, poi_color in zip(poi_positions, poi_colors):\n",
    "        ax.plot(poi_position[0], poi_position[1], marker=\"^\", color=poi_color, markersize=plot_marker_size+5)\n",
    "\n",
    "    # Add axes labels and a title\n",
    "    if individual_plot:\n",
    "        ax.set_xlabel('X', fontsize=20)\n",
    "        ax.set_ylabel('Y', fontsize=20)\n",
    "        ax.set_title('Joint Trajectory')\n",
    "\n",
    "    # Limits according to map dimensions from config\n",
    "    # ax.set_xlim([0, map_dim_x])\n",
    "    # ax.set_ylim([0, map_dim_y])\n",
    "\n",
    "    ax.set_xlim([0, 60])\n",
    "    ax.set_ylim([0, 60])\n",
    "\n",
    "    # Custom legend\n",
    "    # leader_handles = [\n",
    "    #     Line2D([0], [0], color=leader_color, lw=1) for leader_color in leader_colors\n",
    "    # ]\n",
    "    # leader_labels = [\"Leader \"+str(i+1) for i in range(num_leaders)]\n",
    "\n",
    "    label_marker_size = 10\n",
    "\n",
    "    leader_handle = Line2D([0], [0], color=leader_color, lw=1, marker=\"P\", markersize=label_marker_size)\n",
    "    leader_label = \"Leader\"\n",
    "\n",
    "    follower_handle = Line2D([0], [0], color=follower_color, lw=1, marker=\"s\", markersize=label_marker_size)\n",
    "    follower_label = \"Follower\"\n",
    "\n",
    "    poi_handle = Line2D([0], [0], color=poi_observed_color, lw=0, marker=\"^\", markersize=label_marker_size+5)\n",
    "    poi_label = \"POI\"\n",
    "\n",
    "    # handles = leader_handles + [follower_handle] + [poi_handle]\n",
    "    # labels = leader_labels + [follower_label] + [poi_label]\n",
    "\n",
    "    handles = [leader_handle, follower_handle, poi_handle]\n",
    "    labels = [leader_label, follower_label, poi_label]\n",
    "\n",
    "    if individual_plot and show_legend:\n",
    "        ax.legend(handles, labels, ncol=1, fontsize=20)\n",
    "\n",
    "    # Add a grid and make it look pretty\n",
    "    ax.grid()\n",
    "    ax.tick_params(\n",
    "        axis='both',\n",
    "        which='both',\n",
    "        top = False,\n",
    "        bottom = False,\n",
    "        left = False,\n",
    "        right = False,\n",
    "        labelsize=15\n",
    "    )\n",
    "    # Give plot a gray background like ggplot.\n",
    "    ax.set_facecolor('#EBEBEB')\n",
    "    # Remove border around plot.\n",
    "    [ax.spines[side].set_visible(False) for side in ax.spines]\n",
    "    # Style the grid.\n",
    "    ax.grid(which='major', color='white', linewidth=1.2)\n",
    "    ax.grid(which='minor', color='white', linewidth=0.6)\n",
    "\n",
    "    # Set up the ticks for the grid\n",
    "    xticks = np.linspace(0, int(map_dim_x - (map_dim_x%10)), int(map_dim_x/10.)+1)\n",
    "    yticks = np.linspace(0, int(map_dim_y - (map_dim_y%10)), int(map_dim_y/10.)+1)\n",
    "\n",
    "    ax.set_yticks(ticks=yticks, labels=None)\n",
    "    ax.set_xticks(ticks=xticks, labels=None)\n",
    "\n",
    "    # Remove labels for individual ticks if option is specified\n",
    "    if not individual_plot:\n",
    "        ax.set_xticks(ticks=xticks, labels=[])\n",
    "        ax.set_yticks(ticks=yticks, labels=[])\n",
    "\n",
    "    # Show the minor ticks and grid.\n",
    "    # ax.minorticks_on()\n",
    "    # Now hide the minor ticks (but leave the gridlines).\n",
    "    # ax.tick_params(which='minor', bottom=False, left=False)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    # ax.set_aspect(1)\n",
    "\n",
    "def plotJointTrajectory(trial_data: List[Dict], config: Dict, team_id: int | str, generation: Optional[int], plot_save_name: Optional[str] = None, sample_frequency: int = 1, show_legend: bool = True, show_plot: bool = True):\n",
    "    \"\"\"High level function for plotting joint trajectories. \n",
    "    team_id can be an integer representing the training team to plot\n",
    "        or it can be the str Eval representing to plot the joint trajectory of the evaluation team\n",
    "    generation should be an integer specifying which generation's team to use for the joint trajectory\n",
    "        if generation is None then this should plot the joint trajectories from the last generation\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    leader_colors = config[\"CCEA\"][\"config\"][\"BoidsEnv\"][\"config\"][\"Renderer\"][\"leader_colors\"]\n",
    "    leader_colors = tuple(np.array(leader_colors)/255)\n",
    "\n",
    "    if generation is None:\n",
    "        generation = config[\"num_generations\"]\n",
    "\n",
    "    if team_id is None:\n",
    "    # Figure out the grid size to place all of the plots in\n",
    "        # Reference: https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html\n",
    "        num_teams = config[\"CCEA\"][\"sub_population_size\"]+1\n",
    "        grid_len = int(np.ceil(np.sqrt(num_teams)))\n",
    "        fig, axs = plt.subplots(nrows=grid_len, ncols=grid_len, figsize=(15,15), tight_layout=True)\n",
    "        for team_id, ax in zip(np.arange(num_teams), axs.flat):\n",
    "            # Add in case for plotting joint trajectory of the evaluation team\n",
    "            if team_id == num_teams-1:\n",
    "                team_id = \"Eval\"\n",
    "                # Get the fitness of the evaluation team\n",
    "                team_fitness = trial_data[generation][\"evaluation_team\"][\"team_fitness\"][0]\n",
    "            else:\n",
    "                # Get the fitness of the training team\n",
    "                team_fitness = trial_data[generation][\"training_teams\"][\"team_\"+str(team_id)][\"team_fitness\"][0]\n",
    "\n",
    "            plotJointTrajectorySubplot(trial_data=trial_data, config=config, ax=ax, generation=generation, team_id=team_id, individual_plot=False, sample_frequency=sample_frequency)\n",
    "            # Objects for a custom legend that just lets me display important metadata\n",
    "            class AnyObject:\n",
    "                pass\n",
    "            class AnyObjectHandler:\n",
    "                def legend_artist(self, legend, orig_handle, fontsize, handlebox):\n",
    "                    x0, y0 = handlebox.xdescent, handlebox.ydescent\n",
    "                    width, height = handlebox.width, handlebox.height\n",
    "                    patch = mpatches.Rectangle([x0, y0], 0, 0, facecolor='red',\n",
    "                                            edgecolor='black', hatch='xx', lw=0,\n",
    "                                            transform=handlebox.get_transform())\n",
    "                    handlebox.add_artist(patch)\n",
    "                    return patch\n",
    "                \n",
    "            # # Extract the fitness for this particular team\n",
    "            # team_fitness = teams_in_evaluations[generation][team_id].fitness\n",
    "            # team_fitness = trial_data[]\n",
    "\n",
    "            # Format that fitness into a nice str\n",
    "            fitness_str = f\"{team_fitness:.3f}\"\n",
    "\n",
    "            # Custom legend that acts as a label for what team this plot is from\n",
    "            # fake_handle = Line2D([0], [0], color='white', lw=0)\n",
    "            fake_handle = AnyObject()\n",
    "            fake_label = str(team_id) + \" | \" + fitness_str\n",
    "            ax.legend([fake_handle], [fake_label], loc='upper right', handler_map={AnyObject: AnyObjectHandler()}, handlelength=-1)\n",
    "            fig.suptitle(\"Generation \"+str(generation))\n",
    "        if team_id == \"Eval\":\n",
    "            team_id = num_teams+1\n",
    "        for ax in axs.flat[team_id+1:]:\n",
    "            ax.tick_params(\n",
    "                axis='both',\n",
    "                which='both',\n",
    "                top = False,\n",
    "                bottom = False,\n",
    "                left = False,\n",
    "                right = False\n",
    "            )\n",
    "            ax.set_xticks(ticks=[], labels=[])\n",
    "            ax.set_yticks(ticks=[], labels=[])\n",
    "            # Remove border around plot.\n",
    "            [ax.spines[side].set_visible(False) for side in ax.spines]\n",
    "\n",
    "        fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "        if show_plot: \n",
    "            plt.show()\n",
    "        pass\n",
    "    # If team id is specified, then just plot that one joint trajectory\n",
    "    else:\n",
    "        # Get map dimensions for figuring the x and y limits of the graph\n",
    "        map_dimensions = config[\"CCEA\"][\"config\"][\"BoidsEnv\"][\"config\"][\"map_dimensions\"]\n",
    "        map_dim_x = map_dimensions[\"x\"]\n",
    "        map_dim_y = map_dimensions[\"y\"]\n",
    "\n",
    "        # Use map dimensions to figure out correctly proportioned graph size\n",
    "        # Keep x dimension the same and adjust the y dimension accordingly\n",
    "        fig_x = 5.\n",
    "        fig_y = fig_x * float(map_dim_y)/float(map_dim_x)\n",
    "\n",
    "        # Set up the plot\n",
    "        # fig = plt.figure(figsize=(fig_x, fig_y))\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        plotJointTrajectorySubplot(trial_data=trial_data, config=config, ax=ax, generation=generation, team_id=team_id, sample_frequency=sample_frequency, show_legend=show_legend)\n",
    "\n",
    "        if plot_save_name is not None:\n",
    "            png_plot_save_name = \"../../figures/png/\" + plot_save_name + \".png\"\n",
    "            svg_plot_save_name = \"../../figures/svg/\" + plot_save_name + \".svg\"\n",
    "\n",
    "            print(\"Saving plot as \", png_plot_save_name)\n",
    "            print(\"Saving plot as \", svg_plot_save_name)\n",
    "            plt.savefig(png_plot_save_name)\n",
    "            plt.savefig(svg_plot_save_name)\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dfollow trials:  ['trial_59', 'trial_58', 'trial_57', 'trial_56', 'trial_55', 'trial_54', 'trial_53', 'trial_52', 'trial_51', 'trial_50', 'trial_49', 'trial_48', 'trial_47', 'trial_46', 'trial_45', 'trial_44', 'trial_43', 'trial_42', 'trial_41', 'trial_40']\n",
      "D trials:  ['trial_39', 'trial_38', 'trial_37', 'trial_36', 'trial_35', 'trial_34', 'trial_33', 'trial_32', 'trial_31', 'trial_30', 'trial_29', 'trial_28', 'trial_27', 'trial_26', 'trial_25', 'trial_24', 'trial_23', 'trial_22', 'trial_21', 'trial_20']\n",
      "G trials:  ['trial_19', 'trial_18', 'trial_17', 'trial_16', 'trial_15', 'trial_14', 'trial_13', 'trial_12', 'trial_11', 'trial_10', 'trial_9', 'trial_8', 'trial_7', 'trial_6', 'trial_5', 'trial_4', 'trial_3', 'trial_2', 'trial_1', 'trial_0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:02<00:00, 169.82it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 168.08it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 176.58it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 173.94it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 175.44it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 177.49it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 176.25it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 171.23it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 175.07it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 172.77it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 173.06it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 171.53it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 172.85it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 172.34it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 171.41it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 170.59it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 165.96it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 166.11it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 165.50it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 163.94it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 181.93it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 188.16it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 183.24it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 174.10it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 154.24it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 161.11it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 187.22it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 189.56it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 191.81it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 191.01it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 193.53it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 190.58it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 192.93it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 184.29it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 175.30it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 168.16it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 172.70it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 166.90it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 166.51it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 168.36it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 165.93it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 171.05it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 169.52it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 165.92it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 160.24it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 158.88it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 165.78it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 164.00it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 164.07it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 159.27it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 166.39it/s]\n",
      "100%|██████████| 501/501 [00:03<00:00, 166.69it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 168.80it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 170.15it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 167.78it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 167.98it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 172.75it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 171.53it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 172.65it/s]\n",
      "100%|██████████| 501/501 [00:02<00:00, 172.35it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load in all of the data for the evaluation teams for the 5 leaders scattered experiment\"\"\"\n",
    "num_generations, trial_datas_Dfollow, trial_datas_D, trial_datas_G = loadBatch(\n",
    "    computername=\"experiment_9b_5leaders\",\n",
    "    start_trial_num=59,\n",
    "    num_stat_runs=20,\n",
    "    tested_G = True,\n",
    "    tested_D = True,\n",
    "    tested_Dfollow = True,\n",
    "    load_populations = False,\n",
    "    load_evaluation_teams = True,\n",
    "    load_training_teams = False,\n",
    "    abs_results_path = \"/home/gonzaeve/boids/leader-follower/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot the full batch for experiment 9b 5 leaders here\"\"\"\n",
    "# from leaderfollower.plot_helpers import plotBatchPerformance, PerformanceMetric\n",
    "\n",
    "plotBatchPerformance(\n",
    "    trial_datas_G = trial_datas_G,\n",
    "    trial_datas_D = trial_datas_D,\n",
    "    trial_datas_Dfollow = trial_datas_Dfollow,\n",
    "    plot_min_max_range=False,\n",
    "    computername=\"experiment_9b_5leaders\",\n",
    "    performance_metric = PerformanceMetric.EvaluationTeam\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBatchPerformance(\n",
    "    trial_datas_G = trial_datas_G,\n",
    "    trial_datas_D = trial_datas_D,\n",
    "    trial_datas_Dfollow = trial_datas_Dfollow,\n",
    "    plot_min_max_range=True,\n",
    "    computername=\"experiment_9b_5leaders\",\n",
    "    performance_metric = PerformanceMetric.EvaluationTeam\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to see the performance of DI for the different 5 scattered POI trials\n",
    "%matplotlib inline\n",
    "plt.figure(0)\n",
    "\n",
    "for ind, trial_data_Dfollow in enumerate(trial_datas_Dfollow[:10]):\n",
    "    eval_performance = []\n",
    "    for generation in trial_data_Dfollow:\n",
    "        eval_performance.append(generation[\"evaluation_team\"][\"team_fitness\"])\n",
    "    plt.plot(eval_performance)\n",
    "    plt.legend(list(range(ind+1)))\n",
    "print(len(trial_datas_Dfollow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to see the performance of G for the different 5 scattered POI trials\n",
    "%matplotlib inline\n",
    "plt.figure(0)\n",
    "\n",
    "for ind, trial_data_G in enumerate(trial_datas_G[10:]):\n",
    "    eval_performance = []\n",
    "    for generation in trial_data_G:\n",
    "        eval_performance.append(generation[\"evaluation_team\"][\"team_fitness\"])\n",
    "    plt.plot(eval_performance)\n",
    "    plt.legend(list(range(ind+1)))\n",
    "print(len(trial_datas_G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load in all of the data for the evaluation teams for the 15 leaders scattered experiment\"\"\"\n",
    "num_generations_15b, trial_datas_Dfollow_15b, trial_datas_D_15b, trial_datas_G_15b = loadBatch(\n",
    "    computername=\"experiment_9b_15leaders\",\n",
    "    start_trial_num=59,\n",
    "    num_stat_runs=20,\n",
    "    tested_G = True,\n",
    "    tested_D = True,\n",
    "    tested_Dfollow = True,\n",
    "    load_populations = False,\n",
    "    load_evaluation_teams = True,\n",
    "    load_training_teams = False,\n",
    "    abs_results_path = \"/home/gonzaeve/boids/leader-follower/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot the full batch for experiment 9b 15 leaders here\"\"\"\n",
    "# from leaderfollower.plot_helpers import plotBatchPerformance, PerformanceMetric\n",
    "%matplotlib inline\n",
    "plotBatchPerformance(\n",
    "    trial_datas_G = trial_datas_G_15b,\n",
    "    trial_datas_D = trial_datas_D_15b,\n",
    "    trial_datas_Dfollow = trial_datas_Dfollow_15b,\n",
    "    plot_min_max_range=False,\n",
    "    computername=\"experiment_9b_15leaders\",\n",
    "    performance_metric = PerformanceMetric.EvaluationTeam\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load in all of the data for the evaluation teams for the 25 leaders scattered experiment\"\"\"\n",
    "num_generations_25b, trial_datas_Dfollow_25b, trial_datas_D_25b, trial_datas_G_25b = loadBatch(\n",
    "    computername=\"experiment_9b_25leaders\",\n",
    "    start_trial_num=59,\n",
    "    num_stat_runs=20,\n",
    "    tested_G = True,\n",
    "    tested_D = True,\n",
    "    tested_Dfollow = True,\n",
    "    load_populations = False,\n",
    "    load_evaluation_teams = True,\n",
    "    load_training_teams = False,\n",
    "    abs_results_path = \"/home/gonzaeve/boids/leader-follower/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot the full batch for experiment 9b 25 leaders here\"\"\"\n",
    "# from leaderfollower.plot_helpers import plotBatchPerformance, PerformanceMetric\n",
    "\n",
    "plotBatchPerformance(\n",
    "    trial_datas_G = trial_datas_G_25b,\n",
    "    trial_datas_D = trial_datas_D_25b,\n",
    "    trial_datas_Dfollow = trial_datas_Dfollow_25b,\n",
    "    plot_min_max_range=False,\n",
    "    computername=\"experiment_9b_25leaders\",\n",
    "    performance_metric = PerformanceMetric.EvaluationTeam\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBatchPerformance(\n",
    "    trial_datas_G = trial_datas_G_25b,\n",
    "    trial_datas_D = trial_datas_D_25b,\n",
    "    trial_datas_Dfollow = trial_datas_Dfollow_25b,\n",
    "    plot_min_max_range=True,\n",
    "    computername=\"experiment_9b_25leaders\",\n",
    "    performance_metric = PerformanceMetric.EvaluationTeam\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from leaderfollower.data_helpers import getEvalStatistics, getBestStatistics\n",
    "def getSweepStatistics(\n",
    "        trial_datas_sweep: List[List[Dict]],\n",
    "        performance_metric: PerformanceMetric,\n",
    "        generation: Optional[int] = None,\n",
    "    ):\n",
    "\n",
    "    if performance_metric.value == PerformanceMetric.BestTrainingTeam.value:\n",
    "        getStatistics = getBestStatistics\n",
    "    elif performance_metric.value == PerformanceMetric.EvaluationTeam.value:\n",
    "        getStatistics = getEvalStatistics\n",
    "\n",
    "    if generation is None:\n",
    "        generation = -1\n",
    "\n",
    "    # This gets the final value for each of these statistics across\n",
    "    # the different batches\n",
    "    avg_sweep = []\n",
    "    std_dev_sweep = []\n",
    "    upper_err_sweep = []\n",
    "    lower_err_sweep = []\n",
    "    upper_range_sweep = []\n",
    "    lower_range_sweep = []\n",
    "\n",
    "    for trial_datas in trial_datas_sweep:\n",
    "        avg, std_dev, upper_err, lower_err, upper_range, lower_range = getStatistics(trial_datas)\n",
    "        avg_sweep.append(avg[generation])\n",
    "        std_dev_sweep.append(std_dev[generation])\n",
    "        upper_err_sweep.append(upper_err[generation])\n",
    "        lower_err_sweep.append(lower_err[generation])\n",
    "        upper_range_sweep.append(upper_range[generation])\n",
    "        lower_range_sweep.append(lower_range[generation])\n",
    "\n",
    "    return avg_sweep, std_dev_sweep, upper_err_sweep, lower_err_sweep, upper_range_sweep, lower_range_sweep\n",
    "\n",
    "def getAllSweepStatistics(\n",
    "        trial_datas_sweep_G: List[List[Dict]],\n",
    "        trial_datas_sweep_D: List[List[Dict]],\n",
    "        trial_datas_sweep_Dfollow: List[List[Dict]],\n",
    "        performance_metric: PerformanceMetric,\n",
    "        generation: Optional[int] = None       \n",
    "    ):\n",
    "    G_sweep_stats = getSweepStatistics(trial_datas_sweep_G, performance_metric, generation)\n",
    "    D_sweep_stats = getSweepStatistics(trial_datas_sweep_D, performance_metric, generation)\n",
    "    Dfollow_sweep_stats = getSweepStatistics(trial_datas_sweep_Dfollow, performance_metric, generation)\n",
    "    return G_sweep_stats, D_sweep_stats, Dfollow_sweep_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAllSweepStatistics(\n",
    "        trial_datas_sweep_G: List[List[Dict]],\n",
    "        trial_datas_sweep_D: List[List[Dict]],\n",
    "        trial_datas_sweep_Dfollow: List[List[Dict]],\n",
    "        performance_metric: PerformanceMetric,\n",
    "        generations: Optional[List[int]]=None\n",
    "    ):\n",
    "    plt.figure(0)\n",
    "    legend = []\n",
    "    for ind, generation in enumerate(generations):\n",
    "        G_sweep_stats, D_sweep_stats, Dfollow_sweep_stats = getAllSweepStatistics(trial_datas_sweep_G, trial_datas_sweep_D, trial_datas_sweep_Dfollow, performance_metric, generation)\n",
    "        avg_G, std_dev_G, upper_err_G, lower_err_G, upper_range_G, lower_range_G = G_sweep_stats\n",
    "        avg_D, std_dev_D, upper_err_D, lower_err_D, upper_range_D, lower_range_D = D_sweep_stats\n",
    "        avg_Dfollow, std_dev_Dfollow, upper_err_Dfollow, lower_err_Dfollow, upper_range_Dfollow, lower_range_Dfollow = Dfollow_sweep_stats\n",
    "\n",
    "        line_style = \"-\"\n",
    "        if ind == 0:\n",
    "            line_style = \":\"\n",
    "\n",
    "        plt.plot(list(range(len(avg_G))), avg_G, color='tab:blue', linestyle=line_style, linewidth=4)\n",
    "        plt.plot(list(range(len(avg_D))), avg_D, color='tab:orange', linestyle=line_style, linewidth=4)\n",
    "        plt.plot(list(range(len(avg_Dfollow))), avg_Dfollow, color='tab:green', linestyle=line_style, linewidth=4)\n",
    "        legend.append(f\"$G$, {generation} generations\")\n",
    "        legend.append(f\"$D$, {generation} generations\")\n",
    "        legend.append(f\"$D^I$, {generation} generations\")\n",
    "\n",
    "    for ind, generation in enumerate(generations):\n",
    "        G_sweep_stats, D_sweep_stats, Dfollow_sweep_stats = getAllSweepStatistics(trial_datas_sweep_G, trial_datas_sweep_D, trial_datas_sweep_Dfollow, performance_metric, generation)\n",
    "        avg_G, std_dev_G, upper_err_G, lower_err_G, upper_range_G, lower_range_G = G_sweep_stats\n",
    "        avg_D, std_dev_D, upper_err_D, lower_err_D, upper_range_D, lower_range_D = D_sweep_stats\n",
    "        avg_Dfollow, std_dev_Dfollow, upper_err_Dfollow, lower_err_Dfollow, upper_range_Dfollow, lower_range_Dfollow = Dfollow_sweep_stats\n",
    "\n",
    "        line_style = \"-\"\n",
    "        if ind == 0:\n",
    "            line_style = \":\"\n",
    "\n",
    "        plt.fill_between(list(range(len(upper_err_G))), np.array(upper_err_G).flatten(), np.array(lower_err_G).flatten(), alpha=0.2, color='tab:blue')\n",
    "        plt.fill_between(list(range(len(upper_err_D))), np.array(upper_err_D).flatten(), np.array(lower_err_D).flatten(), alpha=0.2, color='tab:orange')\n",
    "        plt.fill_between(list(range(len(upper_err_Dfollow))), np.array(upper_err_Dfollow).flatten(), np.array(lower_err_Dfollow).flatten(), alpha=0.2, color='tab:green')\n",
    "\n",
    "        \n",
    "    plt.legend(legend, fontsize=20)\n",
    "    plt.ylim([0, 1.01])\n",
    "    plt.xlim([0,2])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.xticks(ticks=[0,1,2], labels=[5,15,25], fontsize=15)\n",
    "    plt.yticks(ticks=None, labels=None, fontsize=15)\n",
    "\n",
    "    plot_save_name = \"experiment_9a_sweep\"\n",
    "\n",
    "    png_plot_save_name = \"../../figures/png/\" + plot_save_name + \".png\"\n",
    "    svg_plot_save_name = \"../../figures/svg/\" + plot_save_name + \".svg\"\n",
    "\n",
    "    print(\"Saving plot as \", png_plot_save_name)\n",
    "    print(\"Saving plot as \", svg_plot_save_name)\n",
    "    plt.savefig(png_plot_save_name)\n",
    "    plt.savefig(svg_plot_save_name)\n",
    "\n",
    "plotAllSweepStatistics(\n",
    "    trial_datas_sweep_G = [trial_datas_G, trial_datas_G_15b, trial_datas_G_25b],\n",
    "    trial_datas_sweep_D = [trial_datas_D, trial_datas_D_15b, trial_datas_D_25b],\n",
    "    trial_datas_sweep_Dfollow = [trial_datas_Dfollow, trial_datas_Dfollow_15b, trial_datas_Dfollow_25b],\n",
    "    performance_metric = PerformanceMetric.EvaluationTeam,\n",
    "    generations = [10, 500]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Joint Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the individual performances of each G trial\n",
    "%matplotlib qt\n",
    "G_performances = []\n",
    "for trial_data in trial_datas_G_25b:\n",
    "    performance = [gen_dict[\"evaluation_team\"][\"team_fitness\"][0] for gen_dict in trial_data]\n",
    "    G_performances.append(performance)\n",
    "    plt.plot(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_G_performances = [performance[-1] for performance in G_performances]\n",
    "G_ids = list(range(len(final_G_performances)))\n",
    "print(final_G_performances)\n",
    "print(G_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(final_G_performances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfinal_G_performances = sorted(final_G_performances)\n",
    "sG_ids = [G_id for _, G_id in sorted(zip(final_G_performances, G_ids), key=lambda pair: pair[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sfinal_G_performances)\n",
    "print(sG_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unsorted\")\n",
    "for p, i in zip(final_G_performances, G_ids):\n",
    "    print(i, p)\n",
    "print(\"sorted\")\n",
    "for p, i in zip(sfinal_G_performances, sG_ids):\n",
    "    print(i,p)\n",
    "\n",
    "# Highest scoring joint policies are at the end of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now I need to look through the training for the G at the 75th percintile, which in this list should be element 14\n",
    "print(sfinal_G_performances[14], sG_ids[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the top joint policy in the 75th percentile is id 10 with final score of .243\n",
    "# Looks right from performance curve for G on the 25 pois case. This is actuall pretty close to the mean\n",
    "# Ok so now I should plot the joint trajectory for the evaluation team for this at different points in time\n",
    "# Careful with variable names here - lots of things changing quickly, things may be named wrong\n",
    "trial_data_G_14 = trial_datas_G_25b[10]\n",
    "config_data_G = loadConfigData(trialname=\"trial_14\", computername=\"experiment_9b_25leaders\", abs_results_path=\"/home/gonzaeve/boids/leader-follower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotJointTrajectory(\n",
    "    trial_data=trial_data_G_14,\n",
    "    config=config_data_G,\n",
    "    team_id=\"Eval\",\n",
    "    generation=0,\n",
    "    sample_frequency=10,\n",
    "    plot_save_name=\"25leaders_scattered_G_trial_14_Gen0_evaljt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotJointTrajectory(\n",
    "    trial_data=trial_data_G_14,\n",
    "    config=config_data_G,\n",
    "    team_id=\"Eval\",\n",
    "    generation=1,\n",
    "    sample_frequency=10,\n",
    "    plot_save_name=\"25leaders_scattered_G_trial_14_Gen1_evaljt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotJointTrajectory(\n",
    "    trial_data=trial_data_G_14,\n",
    "    config=config_data_G,\n",
    "    team_id=\"Eval\",\n",
    "    generation=2,\n",
    "    sample_frequency=10,\n",
    "    plot_save_name=\"25leaders_scattered_G_trial_14_Gen2_evaljt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotJointTrajectory(\n",
    "    trial_data=trial_data_G_14,\n",
    "    config=config_data_G,\n",
    "    team_id=\"Eval\",\n",
    "    generation=3,\n",
    "    sample_frequency=10,\n",
    "    plot_save_name=\"25leaders_scattered_G_trial_14_Gen3_evaljt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hmmmm. time to animate\n",
    "%matplotlib inline\n",
    "for g in range(len(trial_data_G_14)):\n",
    "    plotJointTrajectory(\n",
    "        trial_data=trial_data_G_14,\n",
    "        config=config_data_G,\n",
    "        team_id=\"Eval\",\n",
    "        generation=g,\n",
    "        sample_frequency=10,\n",
    "        plot_save_name=f\"25leaders_scattered_G_trial_10_Gen{g}_evaljt\",\n",
    "        show_legend=False,\n",
    "        show_plot=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to do the same with DI\n",
    "DI_performances = []\n",
    "for trial_data in trial_datas_Dfollow_25b:\n",
    "    performance = [gen_dict[\"evaluation_team\"][\"team_fitness\"][0] for gen_dict in trial_data]\n",
    "    DI_performances.append(performance)\n",
    "    plt.plot(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DI_performances = [performance[-1] for performance in DI_performances]\n",
    "DI_ids = list(range(len(final_DI_performances)))\n",
    "\n",
    "sfinal_DI_performances = sorted(final_DI_performances)\n",
    "sDI_ids = [DI_id for _, DI_id in sorted(zip(final_DI_performances, DI_ids), key=lambda pair: pair[0])]\n",
    "for p, i in zip(sfinal_DI_performances, sDI_ids):\n",
    "    print(i,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(final_DI_performances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sDI_ids[14], sfinal_DI_performances[14])\n",
    "# Wow that's over half the score of the 75th percentile with G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data_DI_2 = trial_datas_G_25b[2]\n",
    "config_data_DI = loadConfigData(trialname=\"trial_2\", computername=\"experiment_9b_25leaders\", abs_results_path=\"/home/gonzaeve/boids/leader-follower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to animate\n",
    "%matplotlib inline\n",
    "for g in range(len(trial_data_DI_2)):\n",
    "    plotJointTrajectory(\n",
    "        trial_data=trial_data_DI_2,\n",
    "        config=config_data_DI,\n",
    "        team_id=\"Eval\",\n",
    "        generation=g,\n",
    "        sample_frequency=10,\n",
    "        plot_save_name=f\"25leaders_scattered_DI_trial_2_Gen{g}_evaljt\",\n",
    "        show_legend=False,\n",
    "        show_plot=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the joint trajectories for the 25 pois case, it is actually very difficult to see how DI is helping the system learn\n",
    "# It is strange though that so many of the leaders just sit there\n",
    "# I feel like maybe one of theose heuristics measuring leader-follower influence is a good idea to incorporate\n",
    "# Maybe do something like\n",
    "# In an episode, look at each leader\n",
    "# See if that leader was within 5 units (influence radius) of a follower at a timestep\n",
    "# Aggregate the amount of timesteps a leader was influencing at least one follower\n",
    "# Average accross statistical runs\n",
    "# Compare accross different methods G, DI, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think it would be worthwhile though to look at the joint trajectories for the 5 pois case because I know DI can reach a score of 1.0 on it\n",
    "# And G does not (at least within 500 generations of training)\n",
    "# I bet this is where I can demonstrate how DI cleans up messy feedback\n",
    "\n",
    "# Let's see how much I can streamline into a single cell\n",
    "\n",
    "def get75thPercentileTrial(trial_datas: List[Dict], plot_performance: bool = True):\n",
    "    # 1) Get all of the performances over generations for each trial\n",
    "    all_performances = []\n",
    "    for trial_data in trial_datas:\n",
    "        performance = [gen_dict[\"evaluation_team\"][\"team_fitness\"][0] for gen_dict in trial_data]\n",
    "        all_performances.append(performance)\n",
    "        if plot_performance:\n",
    "            plt.plot(performance)\n",
    "            plt.title(\"Performance\")\n",
    "\n",
    "    # 2) Get the final performances at the end of training and generate corresponding ids for each final performance\n",
    "    final_performances = [performance[-1] for performance in all_performances]\n",
    "    ids = list(range(len(final_performances)))\n",
    "    print(\"Final Performances\")\n",
    "    print(final_performances)\n",
    "    print(ids)\n",
    "\n",
    "    # 3) Sort the final performances and corresponding ids. Lowest performing joint-policy is 0. Highest performing jp is last element\n",
    "    sorted_scores = sorted(final_performances)\n",
    "    sorted_ids = [id_ for _, id_ in sorted(zip(final_performances, ids), key=lambda pair: pair[0])]\n",
    "    print(\"Sorted Performances\")\n",
    "    print(sorted_scores)\n",
    "    print(sorted_ids)\n",
    "\n",
    "    # 4) Get the top of the 75th percentile. There are 20 trials indexed at 0, so this is the 14th highest score\n",
    "    chosen_score = sorted_scores[14]\n",
    "    chosen_id = sorted_ids[14]\n",
    "    print(\"75th Percentile Chosen Score\")\n",
    "    print(\"id, score\")\n",
    "    print(chosen_id, chosen_score)\n",
    "\n",
    "    # 5) Get this trial from the list of all the trials\n",
    "    chosen_trial_data = trial_datas[chosen_id]\n",
    "\n",
    "    return chosen_trial_data\n",
    "\n",
    "def plotTrialEvalJt(trial_data, config_data, plot_save_name_prepend):\n",
    "    # 6) Plot the joint trajectory learned at each generation. Each a as a seperate image\n",
    "    for g in range(len(trial_data)):\n",
    "        plotJointTrajectory(\n",
    "            trial_data=trial_data,\n",
    "            config=config_data,\n",
    "            team_id=\"Eval\",\n",
    "            generation=g,\n",
    "            sample_frequency=10,\n",
    "            plot_save_name=plot_save_name_prepend+f\"_Gen{g}\", # f\"25leaders_scattered_DI_trial_2_Gen{g}_evaljt\",\n",
    "            show_legend=False,\n",
    "            show_plot=False\n",
    "        )\n",
    "\n",
    "def plot75thPercentile(\n",
    "        trial_datas,\n",
    "        config_data,\n",
    "        plot_save_name_prepend\n",
    "    ):\n",
    "    chosen_trial = get75thPercentileTrial(trial_datas)\n",
    "    plotTrialEvalJt(trial_data=chosen_trial, config_data=config_data, plot_save_name_prepend=plot_save_name_prepend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 leaders case, 5 scattered POIs, G and DI\n",
    "config_data = loadConfigData(trialname=\"trial_0\", computername=\"experiment_9b_5leaders\", abs_results_path=\"/home/gonzaeve/boids/leader-follower\")\n",
    "plot75thPercentile(\n",
    "    trial_datas=trial_datas_G,\n",
    "    config_data=config_data,\n",
    "    plot_save_name_prepend=\"5leaders_9b_G_evaljt_75thpercentile\"\n",
    ")\n",
    "plot75thPercentile(\n",
    "    trial_datas=trial_datas_Dfollow,\n",
    "    config_data=config_data,\n",
    "    plot_save_name_prepend=\"5leaders_9b_DI_evaljt_75thpercentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Performances\n",
      "[0.4153, 0.637, 0.3657, 0.451, 0.654, 0.4502, 0.57, 0.559, 0.636, 0.5166, 0.4773, 0.4973, 0.3496, 0.6367, 0.598, 0.434, 0.635, 0.3767, 0.4224, 0.4395]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Sorted Performances\n",
      "[0.3496, 0.3657, 0.3767, 0.4153, 0.4224, 0.434, 0.4395, 0.4502, 0.451, 0.4773, 0.4973, 0.5166, 0.559, 0.57, 0.598, 0.635, 0.636, 0.6367, 0.637, 0.654]\n",
      "[12, 2, 17, 0, 18, 15, 19, 5, 3, 10, 11, 9, 7, 6, 14, 16, 8, 13, 1, 4]\n",
      "75th Percentile Chosen Score\n",
      "id, score\n",
      "14 0.598\n",
      "Final Performances\n",
      "[1.0, 1.0, 0.623, 0.441, 0.825, 0.828, 0.7534, 1.0, 0.612, 0.829, 0.829, 0.5127, 0.8325, 0.3665, 0.663, 0.718, 1.0, 0.7124, 0.8423, 1.0]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Sorted Performances\n",
      "[0.3665, 0.441, 0.5127, 0.612, 0.623, 0.663, 0.7124, 0.718, 0.7534, 0.825, 0.828, 0.829, 0.829, 0.8325, 0.8423, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[13, 3, 11, 8, 2, 14, 17, 15, 6, 4, 5, 9, 10, 12, 18, 0, 1, 7, 16, 19]\n",
      "75th Percentile Chosen Score\n",
      "id, score\n",
      "18 0.8423\n"
     ]
    }
   ],
   "source": [
    "# Get the 75th percentile for the 5 scattered POIs\n",
    "trial_data_G_75th = get75thPercentileTrial(trial_datas=trial_datas_G, plot_performance=False)\n",
    "trial_data_DI_75th = get75thPercentileTrial(trial_datas=trial_datas_Dfollow, plot_performance=False)\n",
    "config_data = loadConfigData(trialname=\"trial_0\", computername=\"experiment_9b_5leaders\", abs_results_path=\"/home/gonzaeve/boids/leader-follower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving plot as  ../../figures/png/G_75th_percentile_jt_exp9b_5leaders__Gen50.png\n",
      "Saving plot as  ../../figures/svg/G_75th_percentile_jt_exp9b_5leaders__Gen50.svg\n",
      "Saving plot as  ../../figures/png/DI_75th_percentile_jt_exp9b_5leaders__Gen50.png\n",
      "Saving plot as  ../../figures/svg/DI_75th_percentile_jt_exp9b_5leaders__Gen50.svg\n",
      "Saving plot as  ../../figures/png/G_75th_percentile_jt_exp9b_5leaders__Gen250.png\n",
      "Saving plot as  ../../figures/svg/G_75th_percentile_jt_exp9b_5leaders__Gen250.svg\n",
      "Saving plot as  ../../figures/png/DI_75th_percentile_jt_exp9b_5leaders__Gen250.png\n",
      "Saving plot as  ../../figures/svg/DI_75th_percentile_jt_exp9b_5leaders__Gen250.svg\n",
      "Saving plot as  ../../figures/png/G_75th_percentile_jt_exp9b_5leaders__Gen500.png\n",
      "Saving plot as  ../../figures/svg/G_75th_percentile_jt_exp9b_5leaders__Gen500.svg\n",
      "Saving plot as  ../../figures/png/DI_75th_percentile_jt_exp9b_5leaders__Gen500.png\n",
      "Saving plot as  ../../figures/svg/DI_75th_percentile_jt_exp9b_5leaders__Gen500.svg\n"
     ]
    }
   ],
   "source": [
    "# Plot those joint trajectories as svgs at 50, 250, 500 generations\n",
    "%matplotlib qt\n",
    "\n",
    "for g in [50, 250, 500]:\n",
    "    # Plot G joint trajectories at 50, 250, 500 gens\n",
    "    plot_save_name_prepend = \"G_75th_percentile_jt_exp9b_5leaders\"\n",
    "    plotJointTrajectory(\n",
    "        trial_data=trial_data_G_75th,\n",
    "        config=config_data,\n",
    "        team_id=\"Eval\",\n",
    "        generation=g,\n",
    "        sample_frequency=20,\n",
    "        plot_save_name=plot_save_name_prepend+f\"_Gen{g}\",\n",
    "        show_legend=True,\n",
    "        show_plot=False\n",
    "    )\n",
    "    # Plot DI joint trajectories at 50, 250, 500 gens\n",
    "    plot_save_name_prepend = \"DI_75th_percentile_jt_exp9b_5leaders\"\n",
    "    plotJointTrajectory(\n",
    "        trial_data=trial_data_DI_75th,\n",
    "        config=config_data,\n",
    "        team_id=\"Eval\",\n",
    "        generation=g,\n",
    "        sample_frequency=20,\n",
    "        plot_save_name=plot_save_name_prepend+f\"_Gen{g}\",\n",
    "        show_legend=True,\n",
    "        show_plot=False\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Graph of Final Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bar graph for the final performances of these trials\n",
    "%matplotlib inline \n",
    "# for quick iteration\n",
    "# %matplotlib qt for closer inspection\n",
    "\n",
    "# Get the eval statistics for the first set of experiments and plot them in a bar graph\n",
    "\n",
    "num_var_arr = np.arange(3)\n",
    "space = 0.1 # space between (different batches of bar plots?)\n",
    "# total_width = (0.2*4) + (space*3) # change 4 to be the number of lines you have per group, lines - 1 for space\n",
    "total_width = (0.2*3) + (space*3)\n",
    "# I think since I am only plotting G, DI, and D, then I should change 4->3\n",
    "pos = num_var_arr * (total_width + space)\n",
    "\n",
    "# Start the legend now and build it out as we plot\n",
    "legend = []\n",
    "\n",
    "# Get the statistics for the sweep. These are the statistics across the trials at a particular generation\n",
    "trial_datas_sweep_G = [trial_datas_G, trial_datas_G_15b, trial_datas_G_25b]\n",
    "trial_datas_sweep_D = [trial_datas_D, trial_datas_D_15b, trial_datas_D_25b]\n",
    "trial_datas_sweep_Dfollow = [trial_datas_Dfollow, trial_datas_Dfollow_15b, trial_datas_Dfollow_25b]\n",
    "\n",
    "\n",
    "# print(len(trial_datas_sweep_G))\n",
    "# print(trial_datas_sweep_G[0])\n",
    "# print(type(trial_datas_sweep_G))\n",
    "# print(trial_datas_sweep_G)\n",
    "\n",
    "G_sweep_stats, D_sweep_stats, Dfollow_sweep_stats = \\\n",
    "    getAllSweepStatistics(trial_datas_sweep_G, trial_datas_sweep_D, trial_datas_sweep_Dfollow, PerformanceMetric.EvaluationTeam, 500)\n",
    "\n",
    "# Break down the stats into avg, err, etc\n",
    "avg_G, std_dev_G, upper_err_G, lower_err_G, upper_range_G, lower_range_G = G_sweep_stats\n",
    "avg_D, std_dev_D, upper_err_D, lower_err_D, upper_range_D, lower_range_D = D_sweep_stats\n",
    "avg_Dfollow, std_dev_Dfollow, upper_err_Dfollow, lower_err_Dfollow, upper_range_Dfollow, lower_range_Dfollow = Dfollow_sweep_stats\n",
    "\n",
    "# Convert everything into the appropriate data types\n",
    "def convert(list_of_arrays):\n",
    "    return np.array([a[0] for a in list_of_arrays])\n",
    "\n",
    "lower_err_G = convert(lower_err_G)\n",
    "upper_err_G = convert(upper_err_G)\n",
    "avg_G = convert(avg_G)\n",
    "\n",
    "lower_err_D = convert(lower_err_D)\n",
    "upper_err_D = convert(upper_err_D)\n",
    "avg_D = convert(avg_D)\n",
    "\n",
    "lower_err_Dfollow = convert(lower_err_Dfollow)\n",
    "upper_err_Dfollow = convert(upper_err_Dfollow)\n",
    "avg_Dfollow = convert(avg_Dfollow)\n",
    "\n",
    "# Start plotting\n",
    "\n",
    "# Plot G\n",
    "pos_G = pos+0.2\n",
    "plt.vlines(pos_G, lower_err_G, upper_err_G, linestyles=\"-\", colors=\"black\", linewidth=2)\n",
    "plt.plot([pos_G-0.05, pos_G + 0.05], [lower_err_G, lower_err_G], color=\"black\", linewidth=2)\n",
    "plt.plot([pos_G-0.05, pos_G + 0.05], [upper_err_G, upper_err_G], color=\"black\", linewidth=2)\n",
    "plt.bar(pos_G, avg_G, color='tab:blue', width=0.2)\n",
    "legend.append(\"$G$\")\n",
    "\n",
    "# Plot D\n",
    "pos_D = pos\n",
    "plt.vlines(pos_D, lower_err_D, upper_err_D, linestyles=\"-\", colors=\"black\", linewidth=2)\n",
    "plt.plot([pos_D-0.05, pos_D + 0.05], [lower_err_D, lower_err_D], color=\"black\", linewidth=2)\n",
    "plt.plot([pos_D-0.05, pos_D + 0.05], [upper_err_D, upper_err_D], color=\"black\", linewidth=2)\n",
    "plt.bar(pos_D, avg_D, color='tab:orange', width=0.2)\n",
    "legend.append(\"$D$\")\n",
    "\n",
    "# Plot DI\n",
    "pos_Dfollow = pos_G+0.2\n",
    "plt.vlines(pos_Dfollow, lower_err_Dfollow, upper_err_Dfollow, linestyles=\"-\", colors=\"black\", linewidth=2)\n",
    "plt.plot([pos_Dfollow-0.05, pos_Dfollow + 0.05], [lower_err_Dfollow, lower_err_Dfollow], color=\"black\", linewidth=2)\n",
    "plt.plot([pos_Dfollow-0.05, pos_Dfollow + 0.05], [upper_err_Dfollow, upper_err_Dfollow], color=\"black\", linewidth=2)\n",
    "plt.bar(pos_Dfollow, avg_Dfollow, color='tab:green', width=0.2)\n",
    "legend.append(r'$D_{Indirect}$')\n",
    "\n",
    "leg = plt.legend(legend, fontsize=20, loc=\"upper right\")\n",
    "# legend becomes black unless you manually modify it\n",
    "leg.legendHandles[0].set_color(\"tab:blue\")\n",
    "leg.legendHandles[1].set_color(\"tab:orange\")\n",
    "leg.legendHandles[2].set_color(\"tab:green\")\n",
    "\n",
    "plt.xticks(pos + 0.2, [\"5\", \"15\", \"25\"], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.grid()\n",
    "plt.ylim([0,1.01])\n",
    "\n",
    "plot_save_name = \"experiment_9b_sweep_bar_plot\"\n",
    "png_plot_save_name = \"../../figures/png/\" + plot_save_name + \".png\"\n",
    "svg_plot_save_name = \"../../figures/svg/\" + plot_save_name + \".svg\"\n",
    "\n",
    "plt.savefig(svg_plot_save_name, format=\"svg\")\n",
    "plt.savefig(png_plot_save_name)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
