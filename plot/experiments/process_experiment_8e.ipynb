{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"/home/gonzaeve/boids/leader-follower\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.file_helper import loadBatch\n",
    "from lib.plot_helpers import plotBatchPerformance, PerformanceMetric\n",
    "import matplotlib\n",
    "font = {\n",
    "        # 'family' : 'Helvetica',\n",
    "        # 'weight' : 'bold',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "# As far as I can tell, matplotlib inline isn't supported for running a jupyter notebook with vscode\n",
    "# But this line should work for interactive figures for working within a jupyter notebook outside vscode\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dfollow trials:  ['trial_8', 'trial_7', 'trial_6']\n",
      "D trials:  ['trial_5', 'trial_4', 'trial_3']\n",
      "G trials:  ['trial_2', 'trial_1', 'trial_0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:30<00:00, 16.22it/s]\n",
      "100%|██████████| 501/501 [00:30<00:00, 16.26it/s]\n",
      "100%|██████████| 501/501 [00:30<00:00, 16.32it/s]\n",
      "100%|██████████| 501/501 [00:30<00:00, 16.42it/s]\n",
      "100%|██████████| 501/501 [00:30<00:00, 16.49it/s]\n",
      "100%|██████████| 501/501 [00:29<00:00, 16.70it/s]\n",
      "100%|██████████| 501/501 [00:29<00:00, 16.90it/s]\n",
      "100%|██████████| 501/501 [00:30<00:00, 16.47it/s]\n",
      "100%|██████████| 501/501 [00:32<00:00, 15.57it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load in the batch data\"\"\"\n",
    "start_trial_num = 8\n",
    "num_stat_runs = 3\n",
    "computername = \"experiment_8e\"\n",
    "\n",
    "tested_G = True\n",
    "tested_D = True\n",
    "tested_Dfollow = True\n",
    "\n",
    "num_generations, trial_datas_Dfollow, trial_datas_D, trial_datas_G = loadBatch(\n",
    "    computername=computername,\n",
    "    start_trial_num=start_trial_num,\n",
    "    num_stat_runs=num_stat_runs,\n",
    "    tested_G=tested_G,\n",
    "    tested_D=tested_D,\n",
    "    tested_Dfollow=tested_Dfollow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving plot as  figures/trial_8 | stat_runs 3 | Evaluation Team | G D Df | std err | experiment_8e.png\n",
      "Saving plot as  figures/trial_8 | stat_runs 3 | Evaluation Team | G D Df | std err | experiment_8e.svg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Plot the evaluation team performance metric\"\"\"\n",
    "plot_min_max_range = False\n",
    "\n",
    "plotBatchPerformance(\n",
    "    trial_datas_G=trial_datas_G,\n",
    "    trial_datas_D=trial_datas_D,\n",
    "    trial_datas_Dfollow=trial_datas_Dfollow,\n",
    "    plot_min_max_range=plot_min_max_range,\n",
    "    start_trial_num=start_trial_num,\n",
    "    num_stat_runs=num_stat_runs,\n",
    "    computername=computername,\n",
    "    performance_metric=PerformanceMetric.EvaluationTeam\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot the best training team performance metric\"\"\"\n",
    "plot_min_max_range = False\n",
    "\n",
    "plotBatchPerformance(\n",
    "    trial_datas_G=trial_datas_G,\n",
    "    trial_datas_D=trial_datas_D,\n",
    "    trial_datas_Dfollow=trial_datas_Dfollow,\n",
    "    plot_min_max_range=plot_min_max_range,\n",
    "    start_trial_num=start_trial_num,\n",
    "    num_stat_runs=num_stat_runs,\n",
    "    computername=computername,\n",
    "    performance_metric=PerformanceMetric.BestTrainingTeam\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" These functions are for parsing the data from loadTrialData() into more easily manageable pieces of data\"\"\" \n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "def getEvalFitnesses(trial_data: dict):\n",
    "    # List of floating point values. Each value is a team fitness\n",
    "    team_fitnesses = []\n",
    "    # List of sublists of floating point values. Each sublist is the agent fitnesses at that generation\n",
    "    agent_fitnesses = []\n",
    "\n",
    "    for generation_data in trial_data:\n",
    "        team_fitnesses.append(generation_data[\"evaluation_team\"][\"team_fitness\"])\n",
    "        agent_fitnesses.append(generation_data[\"evaluation_team\"][\"agent_fitnesses\"])\n",
    "    \n",
    "    return np.array(team_fitnesses), np.array(agent_fitnesses)\n",
    "\n",
    "def getAllEvalFitnesses(trial_datas: List[dict]):\n",
    "    # Get all the team fitness and agent specific fitnesses for the evaluation teams\n",
    "    all_team_fitnesses = []\n",
    "    all_agent_fitnesses = []\n",
    "    for trial_data in trial_datas:\n",
    "        team_fitnesses, agent_fitnesses = getEvalFitnesses(trial_data)\n",
    "        all_team_fitnesses.append(team_fitnesses)\n",
    "        all_agent_fitnesses.append(agent_fitnesses)\n",
    "    return np.array(all_team_fitnesses), np.array(all_agent_fitnesses)\n",
    "\n",
    "def getEvalStatistics(trial_datas: List[dict]):\n",
    "    # This assumes that all of these trials come from the same config\n",
    "    # (Each trial was run with the same exact configuration)\n",
    "\n",
    "    # Get all the team fitnesses and agent fitnesses out\n",
    "    all_team_fitnesses, _ = getAllEvalFitnesses(trial_datas)\n",
    "\n",
    "    # Get statistics accross these runs\n",
    "    avg_team_fitness_arr = np.average(all_team_fitnesses, axis=0)\n",
    "    std_dev_team_fitness_arr = np.std(all_team_fitnesses, axis=0)\n",
    "    upper_err_team_fitness_arr = avg_team_fitness_arr + std_dev_team_fitness_arr/np.sqrt(all_team_fitnesses.shape[0])\n",
    "    lower_err_team_fitness_arr = avg_team_fitness_arr - std_dev_team_fitness_arr/np.sqrt(all_team_fitnesses.shape[0])\n",
    "    upper_range = np.max(all_team_fitnesses, axis=0)\n",
    "    lower_range = np.min(all_team_fitnesses, axis=0)\n",
    "\n",
    "    return avg_team_fitness_arr, std_dev_team_fitness_arr, upper_err_team_fitness_arr, lower_err_team_fitness_arr, upper_range, lower_range\n",
    "\n",
    "def getBestFitnesses(trial_data: dict):\n",
    "    # List of floating point values. Each value is a team fitness\n",
    "    team_fitnesses = []\n",
    "    # List of sublists of floating point values. Each sublist is the agent fitnesses at that generation\n",
    "    # We use the highest score of an agent across its entire population. We dont' make this dependent on whether this particular\n",
    "    # policy was used in the random team that got the best team fitness\n",
    "    agent_fitnesses = []\n",
    "\n",
    "    for generation_data in trial_data:\n",
    "        team_fitnesses = [generation_data[\"training_teams\"][team_name][\"team_fitness\"] for team_name in generation_data[\"training_teams\"].keys()]\n",
    "        best_team_fitness = max(team_fitnesses)\n",
    "\n",
    "        agent_fitnesses = [generation_data[\"training_teams\"][team_name][\"agent_fitnesses\"] for team_name in generation_data[\"training_teams\"].keys()]\n",
    "        best_agent_fitnesses = [max(agent_fitness_list) for agent_fitness_list in agent_fitnesses]\n",
    "\n",
    "        team_fitnesses.append(best_team_fitness)\n",
    "        agent_fitnesses.append(best_agent_fitnesses)\n",
    "\n",
    "    return team_fitnesses, agent_fitnesses\n",
    "\n",
    "def getAllBestFitnesses(trial_datas: List[dict]):\n",
    "    # Get all the team fitness and agent specific best fitnesses for the training teams\n",
    "    all_team_fitnesses = []\n",
    "    all_agent_fitnesses = []\n",
    "    for trial_data in trial_datas:\n",
    "        team_fitnesses, agent_fitnesses = getBestFitnesses(trial_data)\n",
    "        all_team_fitnesses.append(team_fitnesses)\n",
    "        all_agent_fitnesses.append(agent_fitnesses)\n",
    "    return np.array(all_team_fitnesses), np.array(all_agent_fitnesses)\n",
    "\n",
    "def getBestStatistics(trial_datas: List[dict]):\n",
    "    # This assumes that all of these trials come from the same config\n",
    "    # (Each trial was run with the same exact configuration)\n",
    "\n",
    "    # Get all the team fitnesses out\n",
    "    all_team_fitnesses, _ = getAllBestFitnesses(trial_datas)\n",
    "\n",
    "    # Get statistics accross these runs\n",
    "    avg_team_fitness_arr = np.average(all_team_fitnesses, axis=0)\n",
    "    std_dev_team_fitness_arr = np.std(all_team_fitnesses, axis=0)\n",
    "    upper_err_team_fitness_arr = avg_team_fitness_arr + std_dev_team_fitness_arr/np.sqrt(all_team_fitnesses.shape[0])\n",
    "    lower_err_team_fitness_arr = avg_team_fitness_arr - std_dev_team_fitness_arr/np.sqrt(all_team_fitnesses.shape[0])\n",
    "    upper_range = np.max(all_team_fitnesses, axis=0)\n",
    "    lower_range = np.min(all_team_fitnesses, axis=0)\n",
    "\n",
    "    return avg_team_fitness_arr, std_dev_team_fitness_arr, upper_err_team_fitness_arr, lower_err_team_fitness_arr, upper_range, lower_range\n",
    "\n",
    "def getTrialNames(trial_num: int, num_stat_runs: int):\n",
    "    trialnames = []\n",
    "    for i in range(num_stat_runs):\n",
    "        trialnames.append(\"trial_\"+str(trial_num))\n",
    "        trial_num -= 1\n",
    "    return trialnames, trial_num\n",
    "\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "from enum import IntEnum\n",
    "\n",
    "class PerformanceMetric(IntEnum):\n",
    "    BestTrainingTeam = 0\n",
    "    EvaluationTeam = 1\n",
    "\n",
    "def plotStatisticsAvg(avg, color):\n",
    "    num_generations_arr = np.arange(avg.shape[0])\n",
    "    plt.plot(num_generations_arr, avg, color=color)\n",
    "\n",
    "def plotStatisticsRange(upper_dev, lower_dev, upper_range, lower_range, color, plot_min_max_range=False):\n",
    "    num_generations_arr = np.arange(upper_dev.shape[0])\n",
    "    plt.fill_between(num_generations_arr, upper_dev.flatten(), lower_dev.flatten(), alpha=0.2, color=color)\n",
    "    if plot_min_max_range:\n",
    "        plt.fill_between(num_generations_arr, upper_range.flatten(), lower_range.flatten(), alpha=0.2, color=color)\n",
    "\n",
    "def plotBatchPerformance(trial_datas_G: Optional[dict], trial_datas_D: Optional[dict], trial_datas_Dfollow: Optional[dict], \\\n",
    "                        plot_min_max_range: bool, start_trial_num: int, num_stat_runs: int, computername: str, performance_metric: PerformanceMetric):\n",
    "    plt.figure(0)\n",
    "\n",
    "    if performance_metric.value == PerformanceMetric.BestTrainingTeam.value:\n",
    "        getStatistics = getBestStatistics\n",
    "        title = \"Best Training Team\"\n",
    "    elif performance_metric.value == PerformanceMetric.EvaluationTeam.value:\n",
    "        getStatistics = getEvalStatistics\n",
    "        title = \"Evaluation Team\"\n",
    "\n",
    "    # Get statistics for different reward structures\n",
    "    # We plot the baselines first so that D-Indirect is on the top layer\n",
    "    legend = []\n",
    "    if trial_datas_G is not None:\n",
    "        avg_G, std_dev_G, upper_err_G, lower_err_G, upper_G, lower_G = getStatistics(trial_datas_G)\n",
    "        plotStatisticsAvg(avg_G, color='tab:blue')\n",
    "        legend.append(\"$G$\")\n",
    "        num_generations_arr = np.arange(avg_G.shape[0])\n",
    "\n",
    "    if trial_datas_D is not None:\n",
    "        avg_D, std_dev_D, upper_err_D, lower_err_D, upper_D, lower_D = getStatistics(trial_datas_D)\n",
    "        plotStatisticsAvg(avg_D, color='tab:orange')\n",
    "        legend.append(\"$D$\")\n",
    "        num_generations_arr = np.arange(avg_D.shape[0])\n",
    "\n",
    "    if trial_datas_Dfollow is not None:\n",
    "        avg_Df, std_dev_Df, upper_err_Df, lower_err_Df, upper_Df, lower_Df = getStatistics(trial_datas_Dfollow)\n",
    "        plotStatisticsAvg(avg_Df, color='tab:green')\n",
    "        legend.append(r'$D^I$')\n",
    "        num_generations_arr = np.arange(avg_Df.shape[0])\n",
    "\n",
    "    # Automatically figure out how many generations were in here\n",
    "    plt.ylim([0,1.01])\n",
    "    plt.xlim([0,len(num_generations_arr)-1])\n",
    "\n",
    "    # Add the standard error or min max plotting\n",
    "    if trial_datas_G is not None: \n",
    "        plotStatisticsRange(upper_err_G, lower_err_G, upper_G, lower_G, 'tab:blue', plot_min_max_range)\n",
    "    if trial_datas_D is not None:\n",
    "        plotStatisticsRange(upper_err_D, lower_err_D, upper_D, lower_D, 'tab:orange', plot_min_max_range)\n",
    "    if trial_datas_Dfollow is not None:\n",
    "        plotStatisticsRange(upper_err_Df, lower_err_Df, upper_Df, lower_Df, 'tab:green', plot_min_max_range)\n",
    "\n",
    "    plt.legend(legend)\n",
    "\n",
    "    # plt.legend([\"$G$\", \"$D$\", r'$D_{follow}$'])\n",
    "\n",
    "    plt.xlabel(\"Number of Generations\")\n",
    "    plt.ylabel(\"Performance\")\n",
    "    plt.title(title)\n",
    "\n",
    "    # plt.xlim([0,150])\n",
    "\n",
    "    plot_save_name = \"figures/trial_\"+str(start_trial_num)+\" | stat_runs \"+str(num_stat_runs)+\" | \"+title+\" |\"\n",
    "    if trial_datas_G:\n",
    "        plot_save_name += \" G\"\n",
    "    if trial_datas_D:\n",
    "        plot_save_name += \" D\"\n",
    "    if trial_datas_Dfollow:\n",
    "        plot_save_name += \" Df\"\n",
    "    if plot_min_max_range:\n",
    "        plot_save_name += \" | full range\"\n",
    "    else:\n",
    "        plot_save_name += \" | std err\"\n",
    "    plot_save_name += \" | \" + computername\n",
    "    plot_save_name += \".png\"\n",
    "\n",
    "    print(\"Saving plot as \", plot_save_name)\n",
    "    plt.savefig(plot_save_name)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
